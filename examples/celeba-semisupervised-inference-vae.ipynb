{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probtorch: 0.0+06f25f6 torch: 0.3.0.post4 cuda: True\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "import probtorch\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "# TODO: move this into probtorch.util\n",
    "def expand_inputs(f):\n",
    "    @wraps(f)\n",
    "    def g(*args, num_samples=None, **kwargs):\n",
    "        if not num_samples is None:\n",
    "            new_args = []\n",
    "            new_kwargs = {}\n",
    "            for arg in args:\n",
    "                if hasattr(arg, 'expand'):\n",
    "                    new_args.append(arg.expand(num_samples, *arg.size()))\n",
    "                else:\n",
    "                    new_args.append(arg)\n",
    "            for k in kwargs:\n",
    "                arg = kwargs[k]\n",
    "                if hasattr(arg, 'expand'):\n",
    "                    new_args.append(arg.expand(num_samples, *arg.size()))\n",
    "                else:\n",
    "                    new_args.append(arg)\n",
    "            return f(*new_args, num_samples=num_samples, **new_kwargs)\n",
    "        else:\n",
    "            return f(*args, num_samples=None, **kwargs)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Paramters:\n",
    "NUM_LATENT = 64\n",
    "\n",
    "# Training Parameters:\n",
    "NUM_SAMPLES = 1\n",
    "NUM_BATCH = 1024\n",
    "NUM_EPOCHS = 100\n",
    "LABEL_FRACTION = 0.0\n",
    "LEARNING_RATE = 1e-3\n",
    "EPS = 1e-9\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "# LOSS parameters:\n",
    "ALPHA = 0.1\n",
    "BETA = (4.0, 1.0, 0.0, 4.0, 1.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_latent=NUM_LATENT):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),     # 32 x 32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),   # 16 x 16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 4, 2, 1),  # 8 x 8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),  # 4 x 4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 512, 4),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Conv2d(512, num_latent, 1),\n",
    "        )\n",
    "        self.mu = nn.Conv2d(512, num_latent, 1)\n",
    "        self.logvar = nn.Conv2d(512, num_latent, 1)\n",
    "\n",
    "    @expand_inputs\n",
    "    def forward(self, images, labels=None, num_samples=None):\n",
    "        q = probtorch.Trace()\n",
    "        enc = self.enc(images.view(-1, 3, 64, 64))\n",
    "        mu = self.mu(enc).squeeze()\n",
    "        logvar = self.logvar(enc).squeeze()\n",
    "        latent = q.normal(mu, logvar, name='z')  # need to figure out how they set stdev\n",
    "        return q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(x_mean, x, EPS=1e-9):\n",
    "    return - (torch.log(x_mean + EPS) * x + \n",
    "              torch.log(1 - x_mean + EPS) * (1 - x)).sum(-1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_latent=NUM_LATENT):        \n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_latent, 512, 1, 1, 0),  # 1 x 1\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 1, 0),         # 4 x 4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),         # 8 x 8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 128, 4, 2, 1),         # 16 x 16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),          # 32 x 32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1)             # 64 x 64\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, images, q=None, num_samples=None):\n",
    "        p = probtorch.Trace()\n",
    "        b_size = images.size(0)\n",
    "        latent = p.normal(0.0, 1.0, value=q['z'], name='z')\n",
    "        images_mean = self.dec(latent.view(b_size, -1, 1, 1)).view(b_size, 3, 64, 64).sigmoid()\n",
    "        p.loss(binary_cross_entropy, images_mean, images, name='images')\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elbo(q, p, alpha=ALPHA, beta=BETA):\n",
    "    return probtorch.objectives.montecarlo.elbo(q, p, alpha=alpha, beta=beta[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import os \n",
    "DATA_PATH = '../data/celeb-a/resized_img_align_celeba'\n",
    "\n",
    "if not os.path.isdir(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "class CelebADataset(datasets.ImageFolder):\n",
    "    \"\"\"Subclass of ImageFolder specific to celebA\"\"\"\n",
    "    def __init__(self, root, split='train', transform=None):\n",
    "        super(CelebADataset, self).__init__(root + '/' + split, transform=transform)\n",
    "        self.attributes = np.load(root + '/' + split + '/attributes.npy')[()]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, _ = super(CelebADataset, self).__getitem__(index)\n",
    "        img_name = os.path.basename(self.imgs[index][0])\n",
    "        attr = self.attributes[img_name]\n",
    "        return img, torch.Tensor(attr)\n",
    "\n",
    "train_data = torch.utils.data.DataLoader(\n",
    "                CelebADataset(DATA_PATH, split='train', transform=transforms.ToTensor()),\n",
    "                batch_size=NUM_BATCH, shuffle=True)\n",
    "test_data = torch.utils.data.DataLoader(\n",
    "                CelebADataset(DATA_PATH, split='validation', transform=transforms.ToTensor()),\n",
    "                batch_size=NUM_BATCH, shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = Encoder()\n",
    "dec = Decoder()\n",
    "if CUDA:\n",
    "    enc.cuda()\n",
    "    dec.cuda()\n",
    "optimizer =  torch.optim.Adam(list(enc.parameters())+list(dec.parameters()),\n",
    "                              lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, enc, dec, optimizer, \n",
    "          label_mask={}, label_fraction=LABEL_FRACTION):\n",
    "    epoch_elbo = 0.0\n",
    "    enc.train()\n",
    "    dec.train()\n",
    "    for b, (images, labels) in enumerate(data):        \n",
    "        #images = images.view(-1, NUM_PIXELS)\n",
    "        if CUDA:\n",
    "            images = images.cuda()\n",
    "        images = Variable(images)\n",
    "        optimizer.zero_grad()\n",
    "        if b not in label_mask:\n",
    "            label_mask[b] = (random() < label_fraction)\n",
    "        if label_mask[b]:\n",
    "            labels_onehot = torch.zeros(NUM_BATCH, 1)\n",
    "            #labels_onehot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "            #labels_onehot = torch.clamp(labels_onehot, EPS, 1-EPS)\n",
    "            #labels_onehot = Variable(labels_onehot)\n",
    "            if CUDA:\n",
    "                labels_onehot = labels_onehot.cuda()\n",
    "            q = enc(images, labels_onehot, num_samples=NUM_SAMPLES)\n",
    "        else:\n",
    "            q = enc(images, num_samples=NUM_SAMPLES)\n",
    "        p = dec(images, q, num_samples=NUM_SAMPLES)\n",
    "        loss = -elbo(q, p)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if CUDA:\n",
    "            loss = loss.cpu()\n",
    "        epoch_elbo -= loss.data.numpy()[0]\n",
    "    return epoch_elbo / len(train_data.dataset), label_mask\n",
    "\n",
    "\n",
    "def test(data, enc, dec, infer=True):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    epoch_elbo = 0.0\n",
    "    epoch_correct = 0\n",
    "    N = len(test_data.dataset)\n",
    "    for b, (images, labels) in enumerate(data):\n",
    "        if CUDA:\n",
    "            images = images.cuda()\n",
    "        images = Variable(images)\n",
    "        q = enc(images, num_samples=NUM_SAMPLES)\n",
    "        p = dec(images, q, num_samples=NUM_SAMPLES)\n",
    "        batch_elbo = elbo(q, p)\n",
    "        if CUDA:\n",
    "            batch_elbo = batch_elbo.cpu()\n",
    "        epoch_elbo += batch_elbo.data.numpy()[0]\n",
    "\n",
    "    return epoch_elbo / N, epoch_correct / N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train: ELBO -7.5535e+03 (85s) Test: ELBO -7.3293e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 1] Train: ELBO -7.2447e+03 (84s) Test: ELBO -7.2323e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 2] Train: ELBO -7.2632e+03 (84s) Test: ELBO -7.4186e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 3] Train: ELBO -7.2160e+03 (84s) Test: ELBO -7.1839e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 4] Train: ELBO -7.1601e+03 (84s) Test: ELBO -7.2093e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 5] Train: ELBO -7.1757e+03 (84s) Test: ELBO -7.2307e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 6] Train: ELBO -7.1507e+03 (84s) Test: ELBO -7.1459e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 7] Train: ELBO -7.1807e+03 (84s) Test: ELBO -7.1719e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 8] Train: ELBO -7.3905e+03 (84s) Test: ELBO -7.2913e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 9] Train: ELBO -7.2394e+03 (84s) Test: ELBO -7.2258e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 10] Train: ELBO -7.2243e+03 (84s) Test: ELBO -7.2306e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 11] Train: ELBO -7.2071e+03 (84s) Test: ELBO -7.2270e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 12] Train: ELBO -7.1888e+03 (84s) Test: ELBO -7.3224e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 13] Train: ELBO -7.1998e+03 (84s) Test: ELBO -7.2266e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 14] Train: ELBO -7.2632e+03 (85s) Test: ELBO -7.2880e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 15] Train: ELBO -7.2829e+03 (84s) Test: ELBO -7.2848e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 16] Train: ELBO -7.2413e+03 (84s) Test: ELBO -7.2799e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 17] Train: ELBO -7.2462e+03 (84s) Test: ELBO -7.2737e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 18] Train: ELBO -7.2699e+03 (84s) Test: ELBO -7.2668e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 19] Train: ELBO -7.2474e+03 (84s) Test: ELBO -7.2813e+03, Accuracy 0.000 (7s)\n",
      "[Epoch 20] Train: ELBO -7.2697e+03 (84s) Test: ELBO -7.2787e+03, Accuracy 0.000 (7s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from random import random\n",
    "\n",
    "train_elbo_dict = {'ELBO':[], 'Recun Loss': [], 'TC': [], 'PriorZ':[], 'PriorY':[], 'MI':[]}\n",
    "test_elbo_dict = {'ELBO':[], 'Recun Loss': [], 'TC': [], 'PriorZ':[], 'PriorY':[], 'MI':[]}\n",
    "test_acc_list = []\n",
    "mask = {}\n",
    "for e in range(NUM_EPOCHS):\n",
    "    train_start = time.time()\n",
    "    train_elbo, mask = train(train_data, enc, dec, \n",
    "                             optimizer, mask, LABEL_FRACTION)\n",
    "    train_end = time.time()\n",
    "    test_start = time.time()\n",
    "    test_elbo, test_accuracy = test(test_data, enc, dec)\n",
    "    test_end = time.time()\n",
    "    \n",
    "    test_acc_list.append(test_accuracy)\n",
    "    print('[Epoch %d] Train: ELBO %.4e (%ds) Test: ELBO %.4e, Accuracy %0.3f (%ds)' % (\n",
    "            e, train_elbo, train_end - train_start, \n",
    "            test_elbo, test_accuracy, test_end - test_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    plt.tight_layout()\n",
    "\n",
    "test_DL = torch.utils.data.DataLoader(\n",
    "            CelebADataset(DATA_PATH, split='validation', transform=transforms.ToTensor()),\n",
    "            batch_size=15, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, _) in enumerate(test_DL):\n",
    "    if CUDA:\n",
    "        images = images.cuda()\n",
    "    images = Variable(images)\n",
    "    q = enc(images, num_samples=NUM_SAMPLES)\n",
    "    p = dec(images, q, num_samples=NUM_SAMPLES)\n",
    "    recon = p['images'].value\n",
    "    break\n",
    "\n",
    "show(make_grid(torch.cat([images, recon], 0).data.cpu(), nrow=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all the embeddings\n",
    "Zs = []\n",
    "for b, (images, labels) in enumerate(train_data):\n",
    "    enc.eval()\n",
    "    if CUDA:\n",
    "        images = images.cuda()\n",
    "    images = Variable(images, volatile=True)\n",
    "    q = enc(images, num_samples=NUM_SAMPLES)\n",
    "    Zs.append(q['z'].value)\n",
    "\n",
    "Zs = torch.cat(Zs, 0).data.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(NUM_LATENT, NUM_LATENT, figsize=(30, 30), sharex=True)\n",
    "f.suptitle(r'$Z \\ Embeddings$' , fontsize=30)\n",
    "\n",
    "for i in range(NUM_LATENT):\n",
    "    axarr[NUM_LATENT-1,i].set_xlabel(r'$\\mathbf{z_{%d}}$' % i, fontsize=10)\n",
    "    axarr[i,0].set_ylabel(r'$\\mathbf{z_{%d}}$' % i, fontsize=10)\n",
    "    for j in range(NUM_LATENT):\n",
    "        if i==j:\n",
    "            axarr[j,i].hist(Zs[:,i], bins=40)\n",
    "        else:\n",
    "            axarr[j,i].scatter(Zs[:,i],Zs[:,j],alpha=0.5)\n",
    "            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go no further!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see some Features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vary_z2(index, zmin, zmax):\n",
    "    f, axarr = plt.subplots(10,10,figsize=(10,10),sharey=True)\n",
    "    f.suptitle(r'$\\mathbf{z_{%d}} \\ \\  varying$' % index , fontsize=30)\n",
    "    z_range = np.linspace(zmin,zmax,num=10)\n",
    "    \n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            z = torch.zeros(1,10)\n",
    "            z[0,index] = z_range[i]\n",
    "            z = Variable(z.cuda())\n",
    "            null_image = Variable(torch.Tensor(np.zeros((1, 784)))).cuda()\n",
    "            y_hot = np.zeros((1,10))\n",
    "            y_hot[0,j] = 1\n",
    "            y_hot = Variable(torch.FloatTensor(y_hot).cuda())\n",
    "            q_null = {'z': z, 'y':y_hot}\n",
    "            p = dec(null_image, q_null, num_samples=NUM_SAMPLES) \n",
    "            image = p['images']\n",
    "            image = image.value.cpu().data.numpy().reshape(28,28)\n",
    "            axarr[i,j].imshow(image)\n",
    "            axarr[i,j].axis('off')\n",
    "            \n",
    "    return None\n",
    "\n",
    "for style in range(NUM_STYLE):\n",
    "    vary_z2(style, -3, 3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. feature 0: Horizontal angles (look at 3 ,4, and 7)\n",
    "2. feature 1: Y-position of the middle line (look at 4 middle line)\n",
    "3. feature 2: Width\n",
    "4. feature 3: Nothing\n",
    "5. feature 4: Angle \n",
    "6. feature 5: Nothing\n",
    "7. feature 6: Stroke\n",
    "8. feature 7: Not sure\n",
    "9. feature 8: Top circle gets bigger, the other one smaller\n",
    "10. feature 9: Nothing\n",
    "\n",
    "## Features $z_{i}$ vs $z_{j}$ for Digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zi_vs_zj(z_index1, z_index2, zmin=3, zmax=3, num_z=10, digit=0):\n",
    "    f, axarr = plt.subplots(num_z, num_z, figsize=(num_z, num_z), sharey=True)\n",
    "    f.suptitle(r'$Digit: %s$' % digit , fontsize=30)\n",
    "    z_range = np.linspace(zmin,zmax,num=num_z)\n",
    "    \n",
    "    for i in range(num_z):\n",
    "        for j in range(num_z):\n",
    "            z = torch.zeros(1,NUM_STYLE)\n",
    "            z[0,z_index1] = z_range[i]\n",
    "            z[0,z_index2] = z_range[j]\n",
    "            z = Variable(z.cuda())\n",
    "            y_hot = np.zeros((1,NUM_DIGITS))\n",
    "            y_hot[0,digit] = 1\n",
    "            y_hot = Variable(torch.FloatTensor(y_hot).cuda())\n",
    "            q_null = {'z':z, 'y':y_hot}\n",
    "            null_image = Variable(torch.Tensor(np.zeros((1, NUM_PIXELS)))).cuda()\n",
    "            p = dec(null_image, q_null, num_samples=NUM_SAMPLES) \n",
    "            image = p['images']\n",
    "            pixels = int(np.sqrt(NUM_PIXELS))\n",
    "            image = image.value.cpu().data.numpy().reshape(pixels,pixels)\n",
    "            axarr[i,j].imshow(image)\n",
    "            axarr[i,j].axis('off')\n",
    "    f.text(0.52, 0.08, r'$\\mathbf{z_{%d}}$' % z_index2, ha='center', fontsize=20)\n",
    "    f.text(0.09, 0.5, r'$\\mathbf{z_{%d}}$' % z_index1, va='center', rotation='vertical', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "z_index1 = 6\n",
    "z_index2 = 4\n",
    "for digit in range(NUM_DIGITS):\n",
    "    zi_vs_zj(z_index1, z_index2, zmin=-3, zmax=3, num_z=10, digit=digit)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track terms Vs iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "iters = np.arange(NUM_EPOCHS)\n",
    "for label in train_elbo_dict:\n",
    "    plt.plot(iters, train_elbo_dict[label], label=label, linewidth=3)\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.xlabel(\"iter\", fontsize=20)\n",
    "plt.title(\" Training Process\", fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "iters = np.arange(NUM_EPOCHS)\n",
    "for label in test_elbo_dict:\n",
    "    plt.plot(iters, test_elbo_dict[label], label=label, linewidth=3)\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.xlabel(\"iter\", fontsize=20)\n",
    "plt.title(\" Testing Process\", fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchie",
   "language": "python",
   "name": "pytorchie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
